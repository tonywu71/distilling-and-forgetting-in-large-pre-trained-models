from typing import Dict

import torch

from transformers import WhisperProcessor, EvalPrediction
import evaluate

from utils.constants import LOSS_MASK_IDX


def compute_wer_fct(pred: EvalPrediction, processor: WhisperProcessor, normalize: bool=True) -> Dict[str, float]:
    """
    Compute the WER metric in percent for the given predictions and labels.
    Note: Setting `normalize` to `True` (default) will use the Whisper text normalizer.
    
    IMPORTANT: Due to a bug in the HuggingFace implementation of the Whisper, using
    `batch_decode` with `normalize=True` will always use the English normalizer even
    if the language is not English -> see https://github.com/huggingface/transformers/pull/20707
    For the moment, this is not a problem as we are always fine-tuning on English data, and
    the evaluation script doesn't use `batch_decode`.
    """
    
    wer_metric = evaluate.load("wer")
    
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # Replace the padding index with the pad token id to undo the step we applied
    # in the data collator to ignore padded tokens correctly in the loss:
    label_ids[label_ids==LOSS_MASK_IDX] = processor.tokenizer.pad_token_id  # type: ignore
    
    # Decode the predictions:
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, normalize=normalize)  # type: ignore
    
    # Decode the labels:
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True, normalize=normalize)  # type: ignore

    # Compute the WER in percent:
    wer = 100 * wer_metric.compute(references=label_str, predictions=pred_str)  # type: ignore

    return {"wer": wer}


def compute_wer_fct_distil(pred: EvalPrediction, processor: WhisperProcessor, normalize: bool=True) -> Dict[str, float]:
    """
    Compute the WER metric in percent for the given predictions and labels.
    Note: Setting `normalize` to `True` (default) will use the Whisper text normalizer.
    
    This function should be used for distillation.
    """
    
    wer_metric = evaluate.load("wer")
    
    # `pred` has the following attributes:
    # - predictions: Predictions of the model.
    # - label_ids: Targets to be matched.
    
    # `pred.predictions` is a 2-tuple:
    # - 1st element: the predictions of the student model -> (batch_size, seq_len, vocab_size) = (73, 92, 51865)
    # - 2nd element: the embeddings generated after the 2D convolution layers -> (73, 1500, 384)
    #                See `model.model.encoder.embed_positions (embed_positions): Embedding(1500, 384)

    # TODO: If we manage to use `predict_with_generate`, then we can get rid of the following lines.
    #       To see if that's the case, just check the type of pred.predictions.
    import pdb; pdb.set_trace()
    pred_student = pred.predictions[0]
    # TODO: We are currently computing the WER with teacher-forcing while we are interested in the WER of the
    #       predictions generated by the student model with K-beams.
    pred_ids = torch.argmax(torch.Tensor(pred_student), dim=-1)  # type: ignore
    label_ids = pred.label_ids

    # Replace the padding index with the pad token id to undo the step we applied
    # in the data collator to ignore padded tokens correctly in the loss:
    label_ids[label_ids==LOSS_MASK_IDX] = processor.tokenizer.pad_token_id  # type: ignore
    
    # Decode the predictions:
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, normalize=normalize)  # type: ignore
    
    # Decode the labels:
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True, normalize=normalize)  # type: ignore

    # Compute the WER in percent:
    wer = 100 * wer_metric.compute(references=label_str, predictions=pred_str)  # type: ignore

    return {"wer": wer}
