{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb638b37-c3ba-4c60-b618-d7c875a799f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696bd80d-1a91-47ff-8629-632533796683",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed110f5-4077-4816-b73f-0970d2543b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cdda7eb-200b-4a2d-931c-8f0d25041d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.models.whisper import (WhisperTokenizer,\n",
    "                                         WhisperTokenizerFast,\n",
    "                                         WhisperFeatureExtractor,\n",
    "                                         WhisperForConditionalGeneration)\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from dataloader.collator import DataCollatorSpeechSeq2SeqWithPadding\n",
    "from dataloader.preprocessing_train.preprocessing import prepare_dataset_fct\n",
    "from evaluation.eval_dataset_name_to_dataset_group import EVAL_DATASET_NAME_TO_DATASET_GROUP\n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef80ece-17b5-433e-8af3-af2bb8dd580b",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824d6e40-8a8f-4289-b2ce-85e161670ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"openai/whisper-tiny\"\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(pretrained_model_name_or_path)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model_name_or_path)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(pretrained_model_name_or_path, language=\"english\", task=\"transcribe\")\n",
    "\n",
    "model.generate = partial(model.generate, language=\"english\", task=\"transcribe\",\n",
    "                         max_length=255, use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd2af4-0693-41a1-9ae2-01c6fa20a848",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "426134a2-998e-4755-850f-3025fffc3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `CACHE_DIR_AMI` environment variable not set. Using default cache directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ami (/Users/Tony/.cache/huggingface/datasets/edinburghcstr___ami/ihm/0.0.0/0d128d0aa8145d0f16f3d5b4da86c5d5759dbe9e8f947fda04b25edb56442bd5)\n"
     ]
    }
   ],
   "source": [
    "# dataset_name = \"librispeech_dummy\"\n",
    "dataset_name = \"ami\"\n",
    "\n",
    "ds = EVAL_DATASET_NAME_TO_DATASET_GROUP[dataset_name]()[dataset_name]\n",
    "\n",
    "if dataset_name == \"ami\":\n",
    "    ds = ds.select(list(range(32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b253aad8-4eb5-4c2b-9223-c0b21a8cffea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prepare_dataset = partial(prepare_dataset_fct, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "ds = ds.map(prepare_dataset, num_proc=4).with_format(\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31d057-2496-43dd-a7e3-49fb1fd2b8f1",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "623d7bbb-0262-4724-91fc-4c24239ac8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SO EVEN IF IT DOESN'T WORK YOU CAN JIGGERY POKERY AROUND AND MAKE IT WORK\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ds[25]\n",
    "\n",
    "x[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a07c4cd8-4da3-4012-ab82-862190b279e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" So even if it doesn't work, you can jigory poke it around and make it work.\",\n",
       " \" So even if it doesn't work, you can jiggery poke it around and make it work.\",\n",
       " \" So, even if it doesn't work, you can jiggery poke it around and make it work.\"]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(x[\"input_features\"][None, ...], num_beams=3, num_return_sequences=3)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0418903e-3b22-48c9-ae29-c8b0e7f5cb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" So even if it doesn't work, you can jiggerle it up around and make it work.\",\n",
       " \" So even if it doesn't work you can jitter eat pot agree around and make it work.\",\n",
       " \" Even if it doesn't work, you can jigger rip out around and make it work.\"]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(x[\"input_features\"][None, ...], do_sample=True, top_p=0.92, num_return_sequences=3)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f12d47-d754-45d7-aee2-8c93a2e458c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
