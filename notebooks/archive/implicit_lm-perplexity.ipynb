{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd20bdf-6576-490c-b9f9-304b50ebdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea892cd0-272f-4917-883f-40f7e63bff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8531dd9b-d99f-4e3d-be5b-0714ce884202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "from models.whisper_zero_cross_attention import WhisperForConditionalGenerationZeroCrossAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d7b8b-268a-406d-be9f-caa797e0b45a",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4080af57-faf6-4d0b-849a-d636deaf7167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/Users/Tony/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff13017-15d8-47de-a89c-e85b1a4f860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml-pytorch-speech/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "           7062,   465, 21443,    13, 50256]]),\n",
       " [' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "predicted_ids, transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad6602-7772-4fcf-b2f3-1c17023d3433",
   "metadata": {},
   "source": [
    "## Zero cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5b969a-1c79-4d74-aba2-179776aac53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zero_cross_attention = WhisperForConditionalGenerationZeroCrossAttention.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model_zero_cross_attention.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8bfc5c-1f56-4655-b35e-d58a0d6d0263",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d713bea-92c8-40fb-a9b7-9d3732e1e6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.modeling_outputs.Seq2SeqLMOutput,\n",
       " odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state']))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_seq = torch.tensor([processor.tokenizer(\"Hello my name is\", add_special_tokens=False).input_ids])\n",
    "output = model_zero_cross_attention.forward(input_features=input_features,\n",
    "                                            decoder_input_ids=tokenized_seq)\n",
    "type(output), output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bec7b3e-9e87-4dc1-a68a-35400e08659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c72e317-c1cf-487e-b4c1-a11e3edc29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.all(output.encoder_last_hidden_state == 0.).item(), \"Encoder should output a tensor full of 0s.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88c7a1-0f88-415d-9bde-9f527a890f49",
   "metadata": {},
   "source": [
    "### Sentence-completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08b18d-5fb3-4bc4-97ca-1c4d5b36fd09",
   "metadata": {},
   "source": [
    "**Comments:** Because we have no accoustic model, Whisper is unable to use the audio source to decode. Thus, we get some garbage output as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bec5efb-67f1-4a1b-ac2f-481b9841b9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-14.9954],\n",
       "          [ -0.1458],\n",
       "          [ -8.8252],\n",
       "          [ -4.5569],\n",
       "          [ -7.2125],\n",
       "          [ -3.7046]]], grad_fn=<GatherBackward0>),\n",
       " tensor(715.8162, grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User input:\n",
    "input_seq = \"Hello, my name is Tony.\"\n",
    "\n",
    "# Tokenize input sequence:\n",
    "tokenized_seq = torch.tensor([processor.tokenizer(input_seq, add_special_tokens=False).input_ids])\n",
    "\n",
    "# Shift inputs for next-word prediction:\n",
    "decoder_input_ids = tokenized_seq[:, 1:]\n",
    "shifted_left_decoder_input_ids = tokenized_seq[:, :-1]\n",
    "\n",
    "# One-step generation:\n",
    "output = model_zero_cross_attention.forward(input_features=input_features,\n",
    "                                            decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "# Convert logits to log-probabilities:\n",
    "log_prob_all = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "\n",
    "# Take probabilities for the ground-truth tokens:\n",
    "log_prob = log_prob_all.take_along_dim(shifted_left_decoder_input_ids[..., None], dim=-1)\n",
    "\n",
    "# Compute perplexity:\n",
    "perplexity = torch.exp(-log_prob.mean())\n",
    "\n",
    "log_prob, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2944d-4efb-423d-8eba-910f94dc7dea",
   "metadata": {},
   "source": [
    "With an another example. Since the input sequence is garbage, the obtained perplexity should be much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eff89c2-1666-40b1-b83d-3d9c73c27e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-14.3764],\n",
       "          [-34.3825],\n",
       "          [-15.2447],\n",
       "          [ -8.7378],\n",
       "          [ -7.8441],\n",
       "          [-10.8189],\n",
       "          [ -6.8493]]], grad_fn=<GatherBackward0>),\n",
       " tensor(1246996.2500, grad_fn=<ExpBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User input:\n",
    "input_seq = \"mountain no laptop apple sunny cambridge\"\n",
    "\n",
    "# Tokenize input sequence:\n",
    "tokenized_seq = torch.tensor([processor.tokenizer(input_seq, add_special_tokens=False).input_ids])\n",
    "\n",
    "# Shift inputs for next-word prediction:\n",
    "decoder_input_ids = tokenized_seq[:, 1:]\n",
    "shifted_left_decoder_input_ids = tokenized_seq[:, :-1]\n",
    "\n",
    "# One-step generation:\n",
    "output = model_zero_cross_attention.forward(input_features=input_features,\n",
    "                                            decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "# Convert logits to log-probabilities:\n",
    "log_prob_all = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "\n",
    "# Take probabilities for the ground-truth tokens:\n",
    "log_prob = log_prob_all.take_along_dim(shifted_left_decoder_input_ids[..., None], dim=-1)\n",
    "\n",
    "# Compute perplexity:\n",
    "perplexity = torch.exp(-log_prob.mean())\n",
    "\n",
    "log_prob, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ab735-5065-441b-bafe-a10b224c8f7f",
   "metadata": {},
   "source": [
    "### Bonus: Behavior with `generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a135a03b-d000-4801-ba5a-9179c9aa8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input sequence:\n",
    "decoder_input_ids = torch.tensor([processor.tokenizer(\"Hello my name is\", add_special_tokens=False).input_ids])\n",
    "\n",
    "# One-step generation:\n",
    "output = model_zero_cross_attention.forward(input_features=input_features,\n",
    "                                            decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb420d5d-0f7d-4382-9c34-1c7c61d55251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 50362,   314,  1101, 50256]]),\n",
       " [\"<|startoftranscript|><|notimestamps|> I'm<|endoftext|>\"])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate token ids\n",
    "predicted_ids = model_zero_cross_attention.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "predicted_ids, transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c345f1-8b42-420b-9510-cb1d2926b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
