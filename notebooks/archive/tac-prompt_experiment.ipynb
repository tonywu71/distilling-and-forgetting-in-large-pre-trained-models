{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13fede6-b0a8-4049-a94e-fc2b1ee0bb45",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea6c58e-56e1-4a46-9872-1fb46b8f40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742e2c3f-9dfd-459b-9920-19587dec1690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad746412-a963-4e47-937b-6f942e654115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trainer.prompting import get_labels_with_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b77ce-97f8-4f68-a9e6-95e077bf040d",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1717acf-685f-470d-8d13-fb50fb7cbc11",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de81561-1b45-4462-b895-3f28e6e45090",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")  # type: ignore\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "normalizer = processor.tokenizer._normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df4261-5f36-41be-91db-15ff18077e3d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8faee5fd-dac4-4a66-b74e-902d8bf1498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/Users/Tony/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49f4c27-bdf8-46fd-8c7b-34013c6a153a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/Users/Tony/.cache/huggingface/datasets/downloads/extracted/ebb1d3f740add5af71e53b628d8c9c55e64fc2ff14a6ff31de01228adc704d35/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       " 'audio': {'path': '/Users/Tony/.cache/huggingface/datasets/downloads/extracted/ebb1d3f740add5af71e53b628d8c9c55e64fc2ff14a6ff31de01228adc704d35/dev_clean/1272/128104/1272-128104-0000.flac',\n",
       "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
       "         0.0010376 ]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
       " 'speaker_id': 1272,\n",
       " 'chapter_id': 128104,\n",
       " 'id': '1272-128104-0000'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(ds))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37e0e02-188b-4144-8402-4d9087301cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mister quilter is the apostle of the middle classes and we are glad to welcome his gospel',\n",
       " torch.Size([1, 80, 3000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = normalizer(x[\"text\"])  # normalize label\n",
    "input_features = processor(x[\"audio\"][\"array\"], sampling_rate=x[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "label, input_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453ea82-7066-4480-9ebf-e5a95a7eb71c",
   "metadata": {},
   "source": [
    "## Tokenize the labels for teacher-forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8a8783-4982-47e3-acbc-5dc9da79ca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   76,  1964, 31619,   391,   307,   220,  3322, 50244,   295,   220,\n",
       "          3322,  2808,  5359,   293,   321,   366,  5404,   220,  1353,  2928,\n",
       "           702, 14943]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_label = torch.LongTensor(processor.tokenizer(label, add_special_tokens=False).input_ids)\n",
    "\n",
    "# Add batch dim:\n",
    "tokenized_labels = tokenized_label[None, :]\n",
    "\n",
    "tokenized_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0270d-680b-4f59-a4d3-9e0045a5589d",
   "metadata": {},
   "source": [
    "## Add prompts to teacher-forced labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00adb51a-ca66-4b89-b382-3a8ff22d159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 50259), (2, 50359), (3, 50363)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5daf06e2-d737-4c6f-9104-767436a66fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 50265), (2, 50359), (3, 50363)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bc79db-09f7-4863-a746-19e073081a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50265, 50359, 50363,    76,  1964, 31619,   391,   307,   220,\n",
       "          3322, 50244,   295,   220,  3322,  2808,  5359,   293,   321,   366,\n",
       "          5404,   220,  1353,  2928,   702, 14943, 50257]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_with_prompt, n_prefix_tokens_labels, n_suffix_tokens_labels = get_labels_with_prompt(\n",
    "    labels=tokenized_labels, language=\"french\", task=\"transcribe\", tokenizer=processor.tokenizer)\n",
    "\n",
    "labels_with_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc4e601-43f2-475c-8b49-1edee12b6ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|>mister quilter is the apostle of the middle classes and we are glad to welcome his gospel<|endoftext|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(labels_with_prompt, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3392a-997b-49c0-9992-bfd4d9c7919f",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c645ff-cd04-48d5-a798-edc403647867",
   "metadata": {},
   "source": [
    "## Teacher-forced from greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e53552a-f650-49da-ba1a-9043d3466c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c75ca435-8fab-4200-966b-bfd38b0cc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1456,  1804,   368,   635,   417,  8604,   871,   368,   635, 32400,\n",
       "          1030,  4666,  2795,  1443,   263,   373,   892,   287,     6,  1246,\n",
       "           368,   287,     6,  1246,   368,   635,   417,  8604,    13]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate with greedy search - vanilla\n",
    "pred_gen_raw = model.generate(inputs=input_features)\n",
    "pred_gen_str = processor.tokenizer.batch_decode(pred_gen_raw, skip_special_tokens=True, normalize=False)\n",
    "pred_gen = torch.LongTensor(processor.tokenizer.encode(pred_gen_str[0], add_special_tokens=False))[None, :]\n",
    "\n",
    "pred_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1eed06f-b3c7-453e-a1b3-8613a97f6c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Le plus de la chasse est de la classe et nous débrouillons l'air de l'air de la chasse.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_gen, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50eeb2b7-298c-443f-8a06-c68c9377ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50265, 50359, 50363,  1456,  1804,   368,   635,   417,  8604,\n",
       "           871,   368,   635, 32400,  1030,  4666,  2795,  1443,   263,   373,\n",
       "           892,   287,     6,  1246,   368,   287,     6,  1246,   368,   635,\n",
       "           417,  8604,    13, 50257]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_gen_with_prompts, n_prefix_tokens_labels, n_suffix_tokens_labels = get_labels_with_prompt(\n",
    "    labels=pred_gen, language=\"french\", task=\"transcribe\", tokenizer=processor.tokenizer)\n",
    "\n",
    "pred_gen_with_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963f154b-d384-4943-83b7-ae58c767f32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Le plus de la chasse est de la classe et nous débrouillons l'air de l'air de la chasse.<|endoftext|>\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check (`pred_gen_str` won't be used here):\n",
    "pred_gen_str = processor.tokenizer.batch_decode(pred_gen_with_prompts, skip_special_tokens=False, normalize=False)\n",
    "pred_gen_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f8a7c6-e9ef-4144-91ae-1f192e7d170a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50259, 50358, 50363,  1456,  1804,   368,   635,   417,  8604,   871,\n",
       "           368,   635, 32400,  1030,  4666,  2795,  1443,   263,   373,   892,\n",
       "           287,     6,  1246,   368,   287,     6,  1246,   368,   635,   417,\n",
       "          8604,    13, 50257, 50257]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.forward(input_features=input_features,\n",
    "                       decoder_input_ids=pred_gen_with_prompts)\n",
    "logits = output.logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a91311f3-16de-4da3-a7d7-838987056f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|en|><|translate|><|notimestamps|> Le plus de la chasse est de la classe et nous débrouillons l'air de l'air de la chasse.<|endoftext|><|endoftext|>\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26a0d5-a711-4f7a-a146-57641bd1c0a8",
   "metadata": {},
   "source": [
    "**Interesting:** The model recognized that the source is English as it outputed `<|en|>`. Yet, it got teacher-forced to predict `<|fr|>`. At that moment, the model decided the task of interest was `<|translate|>` which makes sense. However, the rest of the prediction is not a translation of the original sentence because of teacher-forcing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4915a-1f7f-4059-bef3-705c585b8048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
