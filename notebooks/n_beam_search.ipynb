{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc234eb-3a7c-4d0d-b9ac-7edf6d4f1852",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e5ed54-59fc-48bb-a8f8-cb776ae661ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/Users/Tony/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e26093-ffe1-4f82-90ee-f90c4d97ac39",
   "metadata": {},
   "source": [
    "## Vanilla N-beam search with `generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff13017-15d8-47de-a89c-e85b1a4f860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "           7062,   465, 21443,    13, 50256]]),\n",
       " [' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, num_beams=3)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "predicted_ids, transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcd1dd-4b38-457d-a88d-9c311c8a238c",
   "metadata": {},
   "source": [
    "## Get top-N sequences from beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68346146-44a2-48a1-a3f9-08f771df2f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "           7062,   465, 21443,    13, 50256],\n",
       "         [50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,   290,   356,   389,  9675,   284,  7062,\n",
       "            465, 21443,    13, 50256, 50256],\n",
       "         [50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "           7062,   465, 23244,    13, 50256]]),\n",
       " [' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.',\n",
       "  ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n",
       "  ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his Gospel.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, num_beams=3, num_return_sequences=3)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "predicted_ids, transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585f2b8-06d4-4d83-92ff-035efe665530",
   "metadata": {},
   "source": [
    "## Retrieve scores from `generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e651b799-6c30-4093-a95b-4177c032622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_features, num_beams=3, num_return_sequences=3, output_scores=True, return_dict_in_generate=True)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4480d-08c2-4f3d-93c1-9872183949e5",
   "metadata": {},
   "source": [
    "### `outputs.scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08a1190-aa73-440c-92f7-6f6f43e2a6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.scores) # corresponds to the length of the output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4feca143-ef7b-4963-9503-4f706931b3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 51864]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf,  ..., -inf, -inf, -inf]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: outputs.scores[0] is not interesting because it would try\n",
    "# to predict the special token \"<|notimestamps|>\".\n",
    "\n",
    "outputs.scores[0].shape, outputs.scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e57339de-c0d9-42cb-a5c9-a9a0e9a534b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 51864]),\n",
       " tensor([[-11.7886,     -inf,     -inf,  ..., -15.2796, -16.0519, -18.6459],\n",
       "         [-11.7886,     -inf,     -inf,  ..., -15.2796, -16.0519, -18.6459],\n",
       "         [-11.7886,     -inf,     -inf,  ..., -15.2796, -16.0519, -18.6459]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.scores[1].shape, outputs.scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a4bd0e7-33b5-4d0e-a8a0-fae6365a1f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 25]),\n",
       " tensor([[50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "           7062,   465, 21443,    13, 50256],\n",
       "         [50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,   290,   356,   389,  9675,   284,  7062,\n",
       "            465, 21443,    13, 50256, 50256],\n",
       "         [50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "            286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "           7062,   465, 23244,    13, 50256]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences.shape, outputs.sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c66016-b315-4ae7-b247-bdd1a1579017",
   "metadata": {},
   "source": [
    "### `outputs.sequences_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4511d882-99a9-4fe4-b888-f749ce0e6c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1096, -0.1257, -0.1332])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences_scores  # score (log-probability) of the whole sentence (best scores from the K beams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93e407-2707-43d3-9b85-3e1fe7a0839b",
   "metadata": {},
   "source": [
    "We are sometimes interested in the transition probability at EACH generation step.\n",
    "To get these with the `generate` method, one can use the `model.compute_transition_scores` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a82ac-1455-4db0-aec3-ba0d7c2bfae0",
   "metadata": {},
   "source": [
    "### `model.compute_transition_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2da09ae1-0ecd-463f-a516-3194cc8d0d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_features, num_beams=3, output_scores=True, return_dict_in_generate=True)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582f92a9-ad59-4220-bd5e-7c69426db3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get only the scores of the words of interest, use the following:\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    sequences=outputs.sequences,\n",
    "    scores=outputs.scores\n",
    ")\n",
    "\n",
    "transition_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec9290-3ac6-4382-81d6-b974e9bdd921",
   "metadata": {},
   "source": [
    "According to the Huggingface [documentation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores), `transition_scores` is:\n",
    "\n",
    "> A torch.Tensor of shape (batch_size*num_return_sequences, sequence_length) containing the transition scores (logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2acda152-5f55-44d3-8c77-80cd8176e2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    -inf, -11.7545, -13.6545, -11.9025, -15.5699, -15.5868, -15.5456,\n",
       "          -9.8802, -11.2776, -16.8050,  -9.5086, -10.5978, -16.2510, -17.7871,\n",
       "         -11.9698, -12.2878,  -9.6142, -15.9646, -15.0512, -11.4573, -13.6455,\n",
       "         -15.4601, -15.6462, -12.2083,  -4.7140],\n",
       "        [    -inf, -11.7545, -11.4174, -19.3821, -16.5451, -16.7485, -15.1044,\n",
       "          -9.5242, -11.3064, -17.6027,  -9.5018, -10.5148, -13.7400, -17.9222,\n",
       "          -7.8407, -13.8596, -12.2393, -13.2752, -17.6306,  -9.5788, -17.5460,\n",
       "         -13.4245, -17.2971, -12.1118,  -0.4220],\n",
       "        [    -inf, -11.7545, -11.3382, -18.9005, -10.6259,  -2.9965, -16.9288,\n",
       "          -9.8166, -11.2027, -16.4502,  -9.4199, -10.7644, -13.8387, -18.0040,\n",
       "         -11.9421, -12.3366, -12.6366, -15.1623, -15.3701, -11.4702, -13.8237,\n",
       "         -15.5517, -16.2082, -10.6575,  -4.6480]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72655c3-6957-4933-b142-d555e2082bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
