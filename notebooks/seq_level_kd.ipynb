{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63fc2f27-fe9e-4161-9b89-5ed66093dbbf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd20bdf-6576-490c-b9f9-304b50ebdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea892cd0-272f-4917-883f-40f7e63bff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8531dd9b-d99f-4e3d-be5b-0714ce884202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a1d7b8b-268a-406d-be9f-caa797e0b45a",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5cd121-e2d9-43a7-9c56-5cfbdd843772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "normalizer = processor.tokenizer._normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f5bde8-f917-49d9-8a38-c70eca46555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/Users/Tony/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "label = normalizer(ds[0][\"text\"])  # normalize label\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff13017-15d8-47de-a89c-e85b1a4f860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e164be-9a16-4291-aa09-a57343ca3f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mister quilter is the apostle of the middle classes and we are glad to welcome his gospel',\n",
       " 'mister quilter is the apostle of the middle classes and we are glad to welcome his gospel')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare pred and label after normalization\n",
    "normalizer(transcription[0]), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1701d769-f49c-46db-85d8-951d317b24f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50362,  1770,    13,  2264,   346,   353,   318,   262, 46329,\n",
       "           286,   262,  3504,  6097,    11,   290,   356,   389,  9675,   284,\n",
       "          7062,   465, 21443,    13, 50256]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a5730dd-3f73-47dc-a7c4-ec06a82106b1",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04075bed-f4fb-496c-8a19-7003cfacf396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50362,    76,  1694,   627,   346,   353,   318,   262, 46329,\n",
       "           286,   262,  3504,  6097,   290,   356,   389,  9675,   284,  7062,\n",
       "           465, 21443, 50256]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize input sequence:\n",
    "tokenized_seq = torch.tensor([processor.tokenizer(label, add_special_tokens=True).input_ids])\n",
    "tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5081c0-ce32-4da1-8da2-9869b1ecbe89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|notimestamps|>mister quilter is the apostle of the middle classes and we are glad to welcome his gospel<|endoftext|>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(tokenized_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f97be03-8f91-421c-967d-5f7028f8009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftranscript|><|notimestamps|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|notimestamps|> Mr']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-step generation:\n",
    "print(processor.tokenizer.batch_decode(tokenized_seq[:, :2]))\n",
    "\n",
    "output = model.forward(input_features=input_features,\n",
    "                       decoder_input_ids=tokenized_seq[:, :2])\n",
    "\n",
    "output_tokenized_seq = torch.argmax(output.logits, dim=-1)\n",
    "processor.tokenizer.batch_decode(output_tokenized_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14727b09-60da-425e-92ee-22bb15d006d4",
   "metadata": {},
   "source": [
    "## Compute $\\mathcal{L}_{\\mathrm{SEQ} - \\mathrm{KD}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f47995-3e8e-40d9-b70f-8ba21b14c4a5",
   "metadata": {},
   "source": [
    "As a reminder: $\\mathcal{L}_{\\mathrm{SEQ} - \\mathrm{KD}} \\approx - \\log p(\\mathbf{t} = \\hat{\\mathbf{t}} \\mid \\mathbf{s})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "903fc5e7-994b-4c23-bc98-9a11a130e4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_output = normalizer(label)  # assume the teacher perfectly transcribes all examples from LibriSpeech\n",
    "# Tokenize input sequence:\n",
    "tokenized_seq = torch.tensor([processor.tokenizer(teacher_output, add_special_tokens=True).input_ids])  # (1, n_tokens)\n",
    "\n",
    "tokenized_seq.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e8d4a02-4869-4d96-b88e-fce6c950e5f8",
   "metadata": {},
   "source": [
    "**Note:** In practice, we should directly use `predicted_ids` obtained with `teacher_model.generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee874455-efea-4c03-8f91-d8ef35e817a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 51864])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-step generation:\n",
    "output = model.forward(input_features=input_features,\n",
    "                       decoder_input_ids=tokenized_seq[:, :-1])  # get rid of the EOT token \"<|endoftext|>\" as generation is supposed to stop here -> (1, n_tokens - 1)\n",
    "\n",
    "log_prob_all = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "log_prob_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968f2ab9-32f3-4e63-a623-fbfc8ac245e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.5336, -16.6798, -14.7979, -15.7406, -15.7755, -15.8578, -15.9635,\n",
       "        -11.9060, -15.5229, -14.4615, -11.9060, -15.8558, -15.9108, -14.8024,\n",
       "        -15.8403, -15.4526, -16.9435, -14.5130, -15.1455, -13.8468, -16.2474,\n",
       "        -13.5256], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_t_hat_step_wise = log_prob_all.take_along_dim(tokenized_seq[:, 1:, None], dim=-1)  # (1, n_tokens - 1)\n",
    "log_prob_t_hat_step_wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e23fa35b-b04d-4d84-b87f-300242df186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-317.2286, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_t_hat = torch.sum(log_prob_t_hat_step_wise)\n",
    "log_prob_t_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3be8f11-29ec-4ba1-b8ab-e1d64e73f3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(log_prob_t_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59daad6e-f38e-42f5-a5e8-1f790a891d76",
   "metadata": {},
   "source": [
    "## Bonus: Recreate the `generate` behavior but with the gradient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "487ff1b7-78b5-48bb-8d1c-e23269ddf248",
   "metadata": {},
   "source": [
    "Note that there is no need to recreate the `generate` behavior for the sequence-level KD as we are only interested in getting the score of $\\mathbf{\\hat t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31fd382f-eab2-46e0-ac8f-e8546008953b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50362]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the sequence\n",
    "tokenized_seq = torch.tensor([processor.tokenizer(\"\", add_special_tokens=True).input_ids])\n",
    "tokenized_seq = tokenized_seq[:, :2]  # get rid of the EOT token \"<|endoftext|>\"\n",
    "tokenized_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9386a78-0c95-46b0-adb1-3d4a32996ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|notimestamps|> Mr']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-step generation:\n",
    "output = model.forward(input_features=input_features,\n",
    "                       decoder_input_ids=tokenized_seq)\n",
    "\n",
    "output_tokenized_seq = torch.argmax(output.logits, dim=-1)\n",
    "processor.tokenizer.batch_decode(output_tokenized_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3eff410-b099-42b2-9367-2cdbe6789473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579e8646-34d4-4b7c-b5dd-eae158c62080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50362, 50362]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([tokenized_seq, output_tokenized_seq[:, -2:-1]], axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9469139-a368-4f2b-957f-496030606f14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Archive (still useful to understand the left-shifted prediction for generative models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffc8d5dd-74d2-4d52-9f17-20912b5e01eb",
   "metadata": {},
   "source": [
    "```python\n",
    "START_OFFSET = 2  # we want to start transcription with \"<|startoftranscript|><|notimestamps|>\"\n",
    "\n",
    "res = []\n",
    "scores = []\n",
    "\n",
    "for idx in range(START_OFFSET, tokenized_seq.shape[1]):  # we add 1 to finish the loop with the full sentence\n",
    "    # One-step generation:\n",
    "    output = model.forward(input_features=input_features,\n",
    "                           decoder_input_ids=tokenized_seq[:, :idx])\n",
    "    \n",
    "    log_prob_all = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "    \n",
    "    output_tokenized_seq = torch.argmax(output.logits, dim=-1)\n",
    "    # scores.append(output.logits[..., output_tokenized_seq])\n",
    "    # scores.append(output.logits.take_along_dim(output_tokenized_seq[..., None], dim=-1))\n",
    "    # scores.append(output.logits.take_along_dim(output_tokenized_seq[..., None], dim=-1))\n",
    "    scores.append(log_prob_all.take_along_dim(tokenized_seq[:, idx]))  # add the score of the ground truth\n",
    "    res.append(processor.tokenizer.batch_decode(output_tokenized_seq))\n",
    "```\n",
    "\n",
    "```\n",
    ">[['<|notimestamps|> Mr'],\n",
    "> ['<|notimestamps|> Mrister'],\n",
    "> ['<|notimestamps|> Mrister Qu'],\n",
    "> ['<|notimestamps|> Mrister Quil'],\n",
    "> ['<|notimestamps|> Mrister Quilter'],\n",
    "> ['<|notimestamps|> Mrister Quilter is'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes,'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to welcome'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to welcome his'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to welcome his gospel']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84da44-ec50-4628-88c3-7eb7c1787ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
