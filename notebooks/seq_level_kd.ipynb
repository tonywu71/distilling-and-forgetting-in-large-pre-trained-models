{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63fc2f27-fe9e-4161-9b89-5ed66093dbbf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd20bdf-6576-490c-b9f9-304b50ebdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea892cd0-272f-4917-883f-40f7e63bff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8531dd9b-d99f-4e3d-be5b-0714ce884202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d7b8b-268a-406d-be9f-caa797e0b45a",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a5cd121-e2d9-43a7-9c56-5cfbdd843772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "normalizer = processor.tokenizer._normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b485908-4e1a-4c98-9f87-1ea6b32db5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/Users/Tony/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b27f9-7e1d-45e3-b133-cdfb694f4141",
   "metadata": {},
   "source": [
    "## Create a 2-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2722c1e4-7de1-44f9-91ee-4bb68fbf9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_0 = ds[0][\"audio\"]\n",
    "label_0 = normalizer(ds[0][\"text\"])  # normalize label\n",
    "input_features_0 = processor(sample_0[\"array\"], sampling_rate=sample_0[\"sampling_rate\"], return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23cfda7-8ac6-4879-9e90-b414c3c2458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = ds[1][\"audio\"]\n",
    "label_1 = normalizer(ds[1][\"text\"])  # normalize label\n",
    "input_features_1 = processor(sample_1[\"array\"], sampling_rate=sample_1[\"sampling_rate\"], return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75df78f-2297-411f-b1c1-0429a186c0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mister quilter is the apostle of the middle classes and we are glad to welcome his gospel',\n",
       " 'nor is mister quilter is manner less interesting than his matter')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_0, label_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82ed70-7a5d-45ce-8a61-ca8b01e53787",
   "metadata": {},
   "source": [
    "### Batch `input_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bfe62f-64c0-46cb-aef0-6c58655371b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert input_features_0.shape == input_features_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e1a13a-a551-45c0-bde8-3d0aad2f30cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80, 3000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features = torch.concat([input_features_0, input_features_1], axis=0)\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca849ab-64b8-4e57-a9cb-39f2604c486a",
   "metadata": {},
   "source": [
    "### Batch `tokenized_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f548f7eb-ee33-4ef3-b47a-152160de7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use `torch.LongTensor` because default dtype is float:\n",
    "tokenized_label_0 = torch.LongTensor(processor.tokenizer(label_0, add_special_tokens=False).input_ids)\n",
    "tokenized_label_1 = torch.LongTensor(processor.tokenizer(label_1, add_special_tokens=False).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f3fec3-391b-4328-ae40-2192073b167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20]), torch.Size([14]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_label_0.shape, tokenized_label_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714073b-bd46-4449-b1ed-60d8afd3ad77",
   "metadata": {},
   "source": [
    "⚠️ The `input_features` share the same shape but this is not true for the tokenized sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546c7d08-4b13-4aa5-8737-af09e1fec4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_features = [{\"input_ids\": tokenized_label_0}, {\"input_ids\": tokenized_label_1}]\n",
    "labels_batch = processor.tokenizer.pad(label_features, return_tensors=\"pt\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7374d38c-32de-4fbc-b90c-2818563593ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   76,  1694,   627,   346,   353,   318,   262, 46329,   286,   262,\n",
       "          3504,  6097,   290,   356,   389,  9675,   284,  7062,   465, 21443],\n",
       "        [13099,   318,   285,  1694,   627,   346,   353,   318,  5642,  1342,\n",
       "          3499,   621,   465,  2300, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f70b3405-7d01-41c6-afed-30f3d903d391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch[\"input_ids\"].shape, labels_batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed4bd9-a502-4319-b089-0c32235358f8",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec1c45-dde9-4445-abc7-8d26a35a81ac",
   "metadata": {},
   "source": [
    "### Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac62f0c-4ff5-4b6a-b9a5-1c00afa23bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 80, 3000]), torch.float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features.shape, input_features.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "014b0327-47dd-430c-95c3-b1f64d2d5003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), torch.int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume the teacher is a perfect model:\n",
    "teacher_sequences = labels_batch[\"input_ids\"]\n",
    "teacher_sequences.shape, teacher_sequences.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6acef8de-2112-4307-9459-ed498b6d5dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invert the attention mask:\n",
    "attention_mask_labels = labels_batch[\"attention_mask\"]\n",
    "attention_mask_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05678b3e-4371-40bd-a91c-f6c0709fc337",
   "metadata": {},
   "source": [
    "### Handle the special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cab36aa-e1f3-42ee-8c4b-e565b7b06b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 25]), torch.Size([2, 25]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = teacher_sequences.size(0)\n",
    "\n",
    "# Get prefix tokens:\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")  # will take care of the EOS token as well\n",
    "prefix_tokens = torch.IntTensor([processor.tokenizer.bos_token_id] + [token_id for idx, token_id in forced_decoder_ids])\n",
    "prefix_tokens = prefix_tokens.expand(batch_size, -1)\n",
    "n_prefix_tokens = prefix_tokens.shape[1]\n",
    "\n",
    "# Get suffix tokens:\n",
    "suffix_tokens = torch.IntTensor([processor.tokenizer.eos_token_id])\n",
    "suffix_tokens = suffix_tokens.expand(batch_size, -1)\n",
    "\n",
    "# Concatenate the prefix tensor with the original tensor along the second dimension\n",
    "teacher_sequences_ = torch.cat((prefix_tokens, teacher_sequences, suffix_tokens), dim=1)\n",
    "labels_ = torch.cat((prefix_tokens, teacher_sequences, suffix_tokens), dim=1)  # should be replaced with `labels` in the final code\n",
    "\n",
    "teacher_sequences_.shape, labels_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9021506-fddc-4daa-96e8-eb95e53153f2",
   "metadata": {},
   "source": [
    "### Predict without attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f5d4894-af32-4412-823d-057464def01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21, 51864])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_no_mask = model.forward(input_features=input_features,\n",
    "                               decoder_input_ids=teacher_sequences_[:, :-1])  # don't predict when current token is EOS\n",
    "logits_no_mask = output_no_mask.logits[:, n_prefix_tokens-1:, :]  # remove what the model tried to predict for the special tokens\n",
    "logits_no_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5afd92-808d-4c57-b31f-9ae3d4fa1fd8",
   "metadata": {},
   "source": [
    "⚠️ To be used with categorical targets, `F.cross_entropy` needs to be used with a tensor for which the 2nd dimension is the class dimension. See [documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e76f1d-a5e9-4d84-aeb2-b54396bb44dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0745, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(input=rearrange(logits_no_mask, pattern=\"b n v -> b v n\"),\n",
    "                target=teacher_sequences_[:, n_prefix_tokens:],\n",
    "                ignore_index=processor.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9d0e0-7315-4da8-8ccc-ffa060ef6b31",
   "metadata": {},
   "source": [
    "### Predict with attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "913534e1-f414-4c8c-af5e-2a2ab2bfb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_prefix = torch.ones(batch_size, n_prefix_tokens)\n",
    "attention_mask_labels_ = torch.cat((attention_prefix, attention_mask_labels), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a4b639e-0604-49e9-b1a5-f37af890c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0745, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_mask = model.forward(input_features=input_features,\n",
    "                                 decoder_input_ids=teacher_sequences_[:, :-1],  # don't predict when current token is EOS\n",
    "                                 decoder_attention_mask=attention_mask_labels_)\n",
    "logits_with_mask = output_with_mask.logits[:, prefix_tokens.size(1)-1:, :]  # remove what the model tried to predict for the special tokens\n",
    "\n",
    "F.cross_entropy(input=rearrange(logits_with_mask, pattern=\"b n v -> b v n\"),\n",
    "                target=teacher_sequences_[:, prefix_tokens.size(1):],\n",
    "                ignore_index=processor.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6ef4e-2b8e-46f2-8d42-b03e9e38031c",
   "metadata": {},
   "source": [
    "### Output comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd0244d6-251b-41d7-bf7d-25891e924585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6267, 8.0974, 5.5852,  ..., 4.2737, 4.8921, 2.6027],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_no_mask[1, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d7f8a1-bbe7-4fcc-92e3-b63bfd5a4fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.0693, 19.4200, 14.3050,  ..., 12.6726, 12.9365, 10.4247],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_with_mask[1, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56e2f089-d1b4-4aef-8bd2-5a74f304ac94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_no_mask == logits_with_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf47e06-8bd3-4e50-b448-61753213cf41",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "- The first first rows ARE equal because our model uses a causal attention mechanism. Therefore, the attention doesn't have the chance to consider the pad tokens at the end of the sequence. Hence the similarity.\n",
    "- The last rows differ for the same reason. Although we could just set them to 0 (which is something we will do eventually), it is good practice to mask them properly. Moreover, we can save some computation time here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af008f85-babd-47b3-8919-86c27485c688",
   "metadata": {},
   "source": [
    "### Apply softmax to get the vocab probabilities per step and per example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95813cfc-2c4d-4a8b-a35d-5a7a8685fb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-17.6328, -16.8028, -17.1586,  ..., -17.6936, -17.4344, -19.5560],\n",
       "         [ -7.2255,  -8.6642, -10.1993,  ..., -10.0005, -10.4366, -11.5987],\n",
       "         [-10.0087,  -8.3535, -13.8741,  ...,  -9.9771,  -9.5403, -11.5162],\n",
       "         ...,\n",
       "         [-13.8001, -12.3024, -16.6508,  ..., -18.9416, -18.2672, -19.6933],\n",
       "         [-11.7289, -12.2270, -15.8037,  ..., -17.4969, -18.7981, -19.3218],\n",
       "         [ -2.9698,  -5.3366, -12.4520,  ..., -15.4419, -15.0755, -17.9387]],\n",
       "\n",
       "        [[-18.5667, -17.4100, -18.5190,  ..., -18.7813, -18.6986, -20.4263],\n",
       "         [-13.3955, -13.2569, -16.5791,  ..., -15.3639, -15.4941, -16.4072],\n",
       "         [ -9.1820,  -8.6636, -12.2782,  ..., -11.1930, -11.2660, -14.2472],\n",
       "         ...,\n",
       "         [ -7.0847,  -7.7393, -12.9708,  ..., -14.5783, -14.3619, -16.8758],\n",
       "         [ -6.9128,  -7.0502, -12.9996,  ..., -14.9758, -14.7272, -17.2178],\n",
       "         [ -7.1692,  -7.8185, -12.9335,  ..., -14.5660, -14.3020, -16.8139]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_log_prob = torch.nn.functional.log_softmax(logits_with_mask, dim=-1)  # (batch_size, n_tokens-1, vocab_size)\n",
    "output_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2502ddc-40b8-4b04-8345-2d071f933756",
   "metadata": {},
   "source": [
    "### Set the values associated to the pad tokens to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141124f-1b6f-4806-a6ad-477f2980f954",
   "metadata": {},
   "source": [
    "First, let's get the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e99f045-0be1-452e-8417-7a994c3f2c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51864"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = output_with_mask.logits.shape[-1]\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893264bf-42da-494c-9ea7-a0f72aa65efd",
   "metadata": {},
   "source": [
    "Because we will sum the log-probabilities to make use of the product rule in the log space, a sufficient method to ignore the padded values is to set them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a2428fd-4511-4429-a608-947027582b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21, 51864])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat attention_mask for the n_vocab dimension:\n",
    "mask = attention_mask_labels_[:, n_prefix_tokens-1:, None].expand(-1, -1, n_vocab)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf21cbdd-cb35-4839-8381-e8150ac556b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-17.6328, -16.8028, -17.1586,  ..., -17.6936, -17.4344, -19.5560],\n",
       "         [ -7.2255,  -8.6642, -10.1993,  ..., -10.0005, -10.4366, -11.5987],\n",
       "         [-10.0087,  -8.3535, -13.8741,  ...,  -9.9771,  -9.5403, -11.5162],\n",
       "         ...,\n",
       "         [-13.8001, -12.3024, -16.6508,  ..., -18.9416, -18.2672, -19.6933],\n",
       "         [-11.7289, -12.2270, -15.8037,  ..., -17.4969, -18.7981, -19.3218],\n",
       "         [ -2.9698,  -5.3366, -12.4520,  ..., -15.4419, -15.0755, -17.9387]],\n",
       "\n",
       "        [[-18.5667, -17.4100, -18.5190,  ..., -18.7813, -18.6986, -20.4263],\n",
       "         [-13.3955, -13.2569, -16.5791,  ..., -15.3639, -15.4941, -16.4072],\n",
       "         [ -9.1820,  -8.6636, -12.2782,  ..., -11.1930, -11.2660, -14.2472],\n",
       "         ...,\n",
       "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_log_prob_masked = output_log_prob.masked_fill(mask.ne(1), 0)\n",
    "output_log_prob_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3e92fe1-d053-4e4f-94c7-28134716d606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.9698,  -5.3366, -12.4520,  ..., -15.4419, -15.0755, -17.9387],\n",
       "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row 1 should be non-null and row 2 should be full of 0s:\n",
    "output_log_prob_masked[:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755516d-fb22-4620-8a72-786b39f11314",
   "metadata": {},
   "source": [
    "## Compute the log-probability of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3133193a-b875-494d-8b10-baf9b6553a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 21, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_t_hat_step_wise = output_log_prob_masked.take_along_dim(teacher_sequences_[:, n_prefix_tokens:, None], dim=-1)\n",
    "log_prob_t_hat_step_wise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0518b045-a7c4-4a8c-9b23-c866b3fa6351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-30.6683, -43.4395], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_t_hat_step_wise.squeeze().sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881dc634-67e2-461c-9ad3-cb772b4efdc3",
   "metadata": {},
   "source": [
    "## Archive (still useful to understand the left-shifted prediction for generative models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00942fc-d6d9-4ac0-a929-203065f16753",
   "metadata": {},
   "source": [
    "```python\n",
    "START_OFFSET = 2  # we want to start transcription with \"<|startoftranscript|><|notimestamps|>\"\n",
    "\n",
    "res = []\n",
    "scores = []\n",
    "\n",
    "for idx in range(START_OFFSET, tokenized_seq.shape[1]):  # we add 1 to finish the loop with the full sentence\n",
    "    # One-step generation:\n",
    "    output = model.forward(input_features=input_features,\n",
    "                           decoder_input_ids=tokenized_seq[:, :idx])\n",
    "    \n",
    "    log_prob_all = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "    \n",
    "    output_tokenized_seq = torch.argmax(output.logits, dim=-1)\n",
    "    # scores.append(output.logits[..., output_tokenized_seq])\n",
    "    # scores.append(output.logits.take_along_dim(output_tokenized_seq[..., None], dim=-1))\n",
    "    # scores.append(output.logits.take_along_dim(output_tokenized_seq[..., None], dim=-1))\n",
    "    scores.append(log_prob_all.take_along_dim(tokenized_seq[:, idx]))  # add the score of the ground truth\n",
    "    res.append(processor.tokenizer.batch_decode(output_tokenized_seq))\n",
    "```\n",
    "\n",
    "```\n",
    ">[['<|notimestamps|> Mr'],\n",
    "> ['<|notimestamps|> Mrister'],\n",
    "> ['<|notimestamps|> Mrister Qu'],\n",
    "> ['<|notimestamps|> Mrister Quil'],\n",
    "> ['<|notimestamps|> Mrister Quilter'],\n",
    "> ['<|notimestamps|> Mrister Quilter is'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes,'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to welcome'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to welcome his'],\n",
    "> ['<|notimestamps|> Mrister Quilter is the apostle of the middle classes, we are glad to welcome his gospel']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0250b-91b3-463d-874a-e08777bbf42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
