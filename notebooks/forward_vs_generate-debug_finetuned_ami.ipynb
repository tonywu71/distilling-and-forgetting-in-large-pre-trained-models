{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13fede6-b0a8-4049-a94e-fc2b1ee0bb45",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea6c58e-56e1-4a46-9872-1fb46b8f40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742e2c3f-9dfd-459b-9920-19587dec1690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad746412-a963-4e47-937b-6f942e654115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trainer.prompting import get_labels_with_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b77ce-97f8-4f68-a9e6-95e077bf040d",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1717acf-685f-470d-8d13-fb50fb7cbc11",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572bf93b-4caf-4261-9663-1f1e0f8609b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dirpath = \"checkpoints/tiny-finetuned_on_ami/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de81561-1b45-4462-b895-3f28e6e45090",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_dirpath)\n",
    "processor = WhisperProcessor.from_pretrained(checkpoint_dirpath)\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")  # type: ignore\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "normalizer = processor.tokenizer._normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df4261-5f36-41be-91db-15ff18077e3d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8faee5fd-dac4-4a66-b74e-902d8bf1498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"edinburghcstr/ami\",\n",
    "                  name=\"ihm\",\n",
    "                  split=\"train\",\n",
    "                  streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49f4c27-bdf8-46fd-8c7b-34013c6a153a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meeting_id': 'EN2001a',\n",
       " 'audio_id': 'AMI_EN2001a_H04_MEO069_0145515_0146152',\n",
       " 'text': \"YEAH IT'LL IT'LL PLAY THEM IN SOME ORDER IN WHICH THEY WERE SET BECAUSE OTHERWISE IT'S GONNA BE MORE ENTERTAINING\",\n",
       " 'audio': {'path': 'EN2001a/train_ami_en2001a_h04_meo069_0145515_0146152.wav',\n",
       "  'array': array([ 0.00000000e+00,  0.00000000e+00,  6.10351562e-05, ...,\n",
       "         -6.10351562e-05, -6.10351562e-05, -3.05175781e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'begin_time': 1455.15,\n",
       " 'end_time': 1461.52,\n",
       " 'microphone_id': 'H04',\n",
       " 'speaker_id': 'MEO069'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_iter = iter(ds)\n",
    "n_skip = 3\n",
    "\n",
    "for _ in range(n_skip):\n",
    "    next(ds_iter)\n",
    "\n",
    "x = next(ds_iter)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37e0e02-188b-4144-8402-4d9087301cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yeah it will it will play them in some order in which they were set because otherwise it is going to be more entertaining',\n",
       " torch.Size([1, 80, 3000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = normalizer(x[\"text\"])  # normalize label\n",
    "input_features = processor(x[\"audio\"][\"array\"], sampling_rate=x[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "label, input_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453ea82-7066-4480-9ebf-e5a95a7eb71c",
   "metadata": {},
   "source": [
    "## Tokenize the labels for teacher-forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8a8783-4982-47e3-acbc-5dc9da79ca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19650,   309,   486,   309,   486,   862,   220, 47959,   294,   512,\n",
       "          1668,   294,   597,   220, 13162,   645,   992,   570,  5911,   309,\n",
       "           307,   516,   220,  1353,   312,   544, 20402]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_label = torch.LongTensor(processor.tokenizer(label, add_special_tokens=False).input_ids)\n",
    "\n",
    "# Add batch dim:\n",
    "tokenized_labels = tokenized_label[None, :]\n",
    "\n",
    "tokenized_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf0270d-680b-4f59-a4d3-9e0045a5589d",
   "metadata": {},
   "source": [
    "## Add prompts to teacher-forced labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5daf06e2-d737-4c6f-9104-767436a66fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 50259), (2, 50359), (3, 50363)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.get_decoder_prompt_ids(language=None, task=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bc79db-09f7-4863-a746-19e073081a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50259, 50359, 50363, 19650,   309,   486,   309,   486,   862,\n",
       "           220, 47959,   294,   512,  1668,   294,   597,   220, 13162,   645,\n",
       "           992,   570,  5911,   309,   307,   516,   220,  1353,   312,   544,\n",
       "         20402, 50257]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_with_prompt, n_prefix_tokens_labels, n_suffix_tokens_labels = get_labels_with_prompt(\n",
    "    labels=tokenized_labels, language=\"english\", task=\"transcribe\", tokenizer=processor.tokenizer)\n",
    "\n",
    "labels_with_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc4e601-43f2-475c-8b49-1edee12b6ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|>yeah it will it will play them in some order in which they were set because otherwise it is going to be more entertaining<|endoftext|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(labels_with_prompt, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3392a-997b-49c0-9992-bfd4d9c7919f",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c645ff-cd04-48d5-a798-edc403647867",
   "metadata": {},
   "source": [
    "## Teacher-forced from greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c75ca435-8fab-4200-966b-bfd38b0cc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[19650,   309,   486,   309,   486,   862,   220, 47959,   294,   512,\n",
       "          1668,   294,   597,   220, 13162,   645,   992,  3082,  5911,   309,\n",
       "           307,   516,   220,  1353,   312,   544, 20402]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate with greedy search - vanilla\n",
    "pred_gen_raw = model.generate(inputs=input_features)\n",
    "pred_gen_str = processor.tokenizer.batch_decode(pred_gen_raw, skip_special_tokens=True, normalize=True)\n",
    "pred_gen = torch.LongTensor(processor.tokenizer.encode(pred_gen_str[0], add_special_tokens=False))[None, :]\n",
    "\n",
    "pred_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1eed06f-b3c7-453e-a1b3-8613a97f6c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_gen, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50eeb2b7-298c-443f-8a06-c68c9377ad32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50259, 50359, 50363, 19650,   309,   486,   309,   486,   862,\n",
       "           220, 47959,   294,   512,  1668,   294,   597,   220, 13162,   645,\n",
       "           992,  3082,  5911,   309,   307,   516,   220,  1353,   312,   544,\n",
       "         20402, 50257]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_gen_with_prompts, n_prefix_tokens_labels, n_suffix_tokens_labels = get_labels_with_prompt(\n",
    "    labels=pred_gen, language=None, task=None, tokenizer=processor.tokenizer)\n",
    "\n",
    "pred_gen_with_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "963f154b-d384-4943-83b7-ae58c767f32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|>yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining<|endoftext|>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check (`pred_gen_str` won't be used here):\n",
    "pred_gen_str = processor.tokenizer.batch_decode(pred_gen_with_prompts, skip_special_tokens=False, normalize=False)\n",
    "pred_gen_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f8a7c6-e9ef-4144-91ae-1f192e7d170a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50359, 50363,  1338,   309,   486,   309,   486,   862,   220,\n",
       "         47959,   294,   512,  1668,   294,   597,   220, 13162,   645,   992,\n",
       "          3082,  5911,   309,   307,   516,   220,  1353,   312,   544, 20402,\n",
       "         50257, 50257]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.forward(input_features=input_features,\n",
    "                       decoder_input_ids=pred_gen_with_prompts)\n",
    "logits = output.logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a91311f3-16de-4da3-a7d7-838987056f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining<|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e66de-0772-44c0-b70d-0eb75093d8ae",
   "metadata": {},
   "source": [
    "## With `generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c9f4680-be84-4b31-90e6-60c4837eb92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 50259, 50359, 50363,  1338,   309,   486,   309,   486,   862,\n",
       "           220, 47959,   294,   512,  1668,   294,   597,   220, 13162,   645,\n",
       "           992,  3082,  5911,   309,   307,   516,   220,  1353,   312,   544,\n",
       "         20402, 50257]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate with greedy search - vanilla\n",
    "pred_gen = model.generate(inputs=input_features)\n",
    "pred_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac27a8d-2a0f-4587-8c0c-5e28c1c72b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining<|endoftext|>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_gen, skip_special_tokens=False, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39965990-a95f-4c74-8082-e72498eddd2f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d87e037e-42ab-49a3-8616-a37dc4cee8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d501a5b7-2341-4afc-a77a-79f64df6621a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(pred_gen, skip_special_tokens=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf0dfe9-0965-4634-b904-3613a0dc692e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah it will it will play them in some order in which they were set because otherwise it is going to be more entertaining'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d965a9f-60aa-4db1-9777-c5d192ae6688",
   "metadata": {},
   "source": [
    "## Bonus: Step-wise teacher-forced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71d74c2-f88a-46b7-8d27-b5d483695406",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for idx in range(1, pred_gen_with_prompts.shape[1]):  # we add 1 to finish the loop with the full sentence\n",
    "    # One-step generation:\n",
    "    output = model.forward(input_features=input_features,\n",
    "                           decoder_input_ids=pred_gen_with_prompts[:, :idx])\n",
    "    \n",
    "    log_prob_all = torch.nn.functional.log_softmax(output.logits, dim=-1)\n",
    "    \n",
    "    output_tokenized_seq = torch.argmax(output.logits, dim=-1)\n",
    "    res.append(processor.tokenizer.batch_decode(output_tokenized_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3cdff63-58d4-48cc-9a03-b0bdacfb6bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<|startoftranscript|>'],\n",
       " ['<|startoftranscript|><|transcribe|>'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|>'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play '],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which '],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going '],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to be'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to be more'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining'],\n",
       " ['<|startoftranscript|><|transcribe|><|notimestamps|> yeah it will it will play them in some order in which they were set cause otherwise it is going to be more entertaining<|endoftext|>']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4915a-1f7f-4059-bef3-705c585b8048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
