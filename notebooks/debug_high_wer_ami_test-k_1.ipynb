{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37324580-06a0-4be8-8194-70b8f85523c3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb50902-acec-4fe8-9681-d944478c0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba60d42-4585-4578-b142-c703099bd091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b422c568-7362-4361-8052-1b78696a1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.models.whisper import (WhisperTokenizer,\n",
    "                                         WhisperTokenizerFast,\n",
    "                                         WhisperFeatureExtractor,\n",
    "                                         WhisperForConditionalGeneration)\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from dataloader.dataset_loader import gen_from_dataset\n",
    "from dataloader.dataset_for_evaluation.ami_test import AMITestSet\n",
    "from evaluation.string_edit_metrics import get_string_edit_metrics\n",
    "\n",
    "device = torch.device('mps')\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcde89-6579-4200-bb8c-f0d0f618971a",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb40d39-119f-4c6a-b960-99dc206d6350",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44a8a39-776d-4177-ae70-33d876edefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"openai/whisper-tiny\"\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(pretrained_model_name_or_path)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model_name_or_path)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(pretrained_model_name_or_path, language=\"english\", task=\"transcribe\")\n",
    "\n",
    "\n",
    "model.config.forced_decoder_ids = tokenizer.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")  # type: ignore\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "whisper_norm = tokenizer._normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482c627-cd0a-41ba-8e61-96bb22b02956",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bcf1c7-5715-4c7e-8537-4dd0c06233e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `CACHE_DIR_AMI` environment variable not set. Using default cache directory.\n"
     ]
    }
   ],
   "source": [
    "ds_group = AMITestSet(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af0dc52-2982-4376-ba18-6baf48666b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x17c035540>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds_group.str2dataset[\"ami\"]\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514824b-98c9-407e-a4ff-e800c3aaba75",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ed08a2-7488-4eda-858e-b72410c214ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "                       model=model,\n",
    "                       tokenizer=tokenizer,\n",
    "                       feature_extractor=feature_extractor,  # type: ignore\n",
    "                       device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157d5ba-883a-4ed9-9384-695eaeda6ecb",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13c4788-c27f-47ee-9acc-3d65d519fd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebd0fea8cb34f38a9413d0b116fd92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:2396: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if unfinished_sequences.max() == 0:\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "n_samples = 200\n",
    "\n",
    "# Create placeholders for the predictions and references:\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for out in tqdm(whisper_asr(gen_from_dataset(ds),\n",
    "                            batch_size=16,\n",
    "                            generate_kwargs={\"num_beams\": 1}),\n",
    "                total=n_samples):\n",
    "    ref = whisper_norm(out[\"reference\"][0])\n",
    "    pred = whisper_norm(out[\"text\"])\n",
    "\n",
    "    if not ref.strip():\n",
    "        continue  # skip empty references to avoid error in WER computation\n",
    "    \n",
    "    predictions.append(pred)\n",
    "    references.append(ref)\n",
    "    \n",
    "    count += 1\n",
    "    if count >= n_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3ab6f-d53d-4409-8065-2258b7d56ca9",
   "metadata": {},
   "source": [
    "## Compute string edit metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3329428-deee-480f-9dee-e8174f42d02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30985915492957744"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53cbab69-30ed-4788-be0d-037130fef0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 0.30985915492957744,\n",
       " 'sub': 0.16735708367854185,\n",
       " 'del': 0.09113504556752279,\n",
       " 'ins': 0.05136702568351284}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_string_edit_metrics(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e5607-d258-4933-ac43-a99df2315ace",
   "metadata": {},
   "source": [
    "## Per-example analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30177442-baa5-4ce4-8e34-14b5b02206b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  18\n",
      "prediction:  thanks for watching\n",
      "reference:  yeah\n",
      "3.0\n",
      "\n",
      "idx:  57\n",
      "prediction:  you are so funny\n",
      "reference:  so\n",
      "3.0\n",
      "\n",
      "idx:  146\n",
      "prediction:  0 it is heavy\n",
      "reference:  ooh\n",
      "4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (prediction, reference) in enumerate(zip(predictions, references)):\n",
    "    wer = get_string_edit_metrics(predictions=[prediction], references=[reference])[\"wer\"]\n",
    "    if wer > 2:\n",
    "        print(\"idx: \", idx)\n",
    "        print(\"prediction: \", prediction)\n",
    "        print(\"reference: \", reference)\n",
    "        print(wer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9649b7ae-7476-4568-b0d4-5120546b9801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  117\n",
      "prediction:  yeah\n",
      "reference:  yeah\n",
      "0.0\n",
      "\n",
      "idx:  152\n",
      "prediction:  you\n",
      "reference:  yeah\n",
      "1.0\n",
      "\n",
      "idx:  156\n",
      "prediction:  hahaha\n",
      "reference:  yeah\n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_of_interest = [117, 152, 156]\n",
    "for idx, (prediction, reference) in enumerate(zip(predictions, references)):\n",
    "    wer = get_string_edit_metrics(predictions=[prediction], references=[reference])[\"wer\"]\n",
    "    if idx in idx_of_interest:\n",
    "        print(\"idx: \", idx)\n",
    "        print(\"prediction: \", prediction)\n",
    "        print(\"reference: \", reference)\n",
    "        print(wer)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819bd092-a031-4615-a33a-4f6d2523a08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
