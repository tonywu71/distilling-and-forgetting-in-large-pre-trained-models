{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb638b37-c3ba-4c60-b618-d7c875a799f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696bd80d-1a91-47ff-8629-632533796683",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed110f5-4077-4816-b73f-0970d2543b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdda7eb-200b-4a2d-931c-8f0d25041d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.models.whisper import (WhisperTokenizer,\n",
    "                                         WhisperTokenizerFast,\n",
    "                                         WhisperFeatureExtractor,\n",
    "                                         WhisperForConditionalGeneration)\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from dataloader.collator import DataCollatorSpeechSeq2SeqWithPadding\n",
    "from dataloader.preprocessing_train.preprocessing import prepare_dataset_fct\n",
    "from evaluation.eval_dataset_name_to_dataset_group import EVAL_DATASET_NAME_TO_DATASET_GROUP\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "elif torch.backends.mps.is_available():  # for Apple Silicon\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef80ece-17b5-433e-8af3-af2bb8dd580b",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "824d6e40-8a8f-4289-b2ce-85e161670ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"openai/whisper-tiny\"\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(pretrained_model_name_or_path).to(device)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(pretrained_model_name_or_path, language=\"english\", task=\"transcribe\")\n",
    "\n",
    "model.generate = partial(model.generate, language=\"english\", task=\"transcribe\",\n",
    "                         max_length=255, use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5461433-7cd2-4502-9692-7aba4e64756f",
   "metadata": {},
   "source": [
    "## Load LogitProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9352fe56-fe3a-4da2-b2b5-e2a165d0015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.logits_process import NoRepeatNGramLogitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3f644a2-cef7-425e-9c59-ee54ce0df0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_processor = NoRepeatNGramLogitsProcessor(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb21b4e-9391-42e4-8dee-27f2cc8b002e",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba25f23a-3a1f-4504-9d42-b3831cd13849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `CACHE_DIR_AMI` environment variable not set. Using default cache directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ami (/Users/Tony/.cache/huggingface/datasets/edinburghcstr___ami/ihm/0.0.0/0d128d0aa8145d0f16f3d5b4da86c5d5759dbe9e8f947fda04b25edb56442bd5)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ami_validation\"\n",
    "\n",
    "ds = EVAL_DATASET_NAME_TO_DATASET_GROUP[dataset_name]()[dataset_name]\n",
    "\n",
    "if dataset_name != \"librispeech_dummy\":\n",
    "    ds = ds.select([3680])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3746c296-f7f3-409a-9d6f-d890700ddd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prepare_dataset = partial(prepare_dataset_fct, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "ds = ds.map(lambda x: {\"text\": x[\"text\"].lower()})\n",
    "ds = ds.map(prepare_dataset, num_proc=4).with_format(\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31d057-2496-43dd-a7e3-49fb1fd2b8f1",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35a7b8d4-57a7-4e25-9526-792c49bcf860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(tokenizer=tokenizer,\n",
    "                                                     feature_extractor=feature_extractor,\n",
    "                                                     replace_padded_with_loss_mask_for_labels=True,\n",
    "                                                     discard_first_bos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fab1a7cb-1382-456a-830e-f0530a236182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'okay yeah yeah yeah yeah yeah that was really horrible',\n",
       " 'audio': {'path': None,\n",
       "  'array': tensor([-0.0080, -0.0015, -0.0049,  ...,  0.0082,  0.0081,  0.0004]),\n",
       "  'sampling_rate': tensor(16000)},\n",
       " 'input_features': tensor([[ 0.4937,  0.3521,  0.4399,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [ 0.2797,  0.1397,  0.3037,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [ 0.0123,  0.2742,  0.5504,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         ...,\n",
       "         [-0.2306, -0.3587, -0.4473,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [-0.3578, -0.4249, -0.4147,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [-0.4052, -0.4152, -0.4409,  ..., -0.4473, -0.4473, -0.4473]]),\n",
       " 'labels': tensor([50258, 50259, 50359, 50363, 26061,  1338,  1338,  1338,  1338,  1338,\n",
       "           220,  6780,   390,   534,  9263, 50257])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ds[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "154f1f5b-43b3-44af-bf22-e67925d64f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Okay, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(x[\"input_features\"][None, ...].to(device), output_scores=True, return_dict_in_generate=True)\n",
    "tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e2a29e29-1db4-448c-97fa-a6c1acde1454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50258, 50259, 50359, 50363,  1033,    11,  1338,    11,  1338,    11,\n",
       "         1338,    11,  1338,    11,  1338,    11,  1338,    11,  1338,    11],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences[0, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0cf783d1-b3b8-4cd2-85f2-9eadf8612aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' yeah,'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1338, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6fcc2d4b-5630-4033-8eab-2b1781e5fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data_collator([x]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c09665e-8724-46cc-8103-8867a6fd175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[ 0.4937,  0.3521,  0.4399,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [ 0.2797,  0.1397,  0.3037,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [ 0.0123,  0.2742,  0.5504,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         ...,\n",
       "         [-0.2306, -0.3587, -0.4473,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [-0.3578, -0.4249, -0.4147,  ..., -0.4473, -0.4473, -0.4473],\n",
       "         [-0.4052, -0.4152, -0.4409,  ..., -0.4473, -0.4473, -0.4473]]],\n",
       "       device='mps:0'), 'labels': tensor([[50259, 50359, 50363, 26061,  1338,  1338,  1338,  1338,  1338,   220,\n",
       "          6780,   390,   534,  9263, 50257]], device='mps:0')}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d820540e-4bd1-4dbb-be47-26df999bae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.forward(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "10f12d47-d754-45d7-aee2-8c93a2e458c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7393,  0.1077,  3.5260,  ...,  1.8835,  2.4299,  3.7835],\n",
       "         [-5.8741, -8.5107, -5.6013,  ..., -5.7708, -5.8595, -4.3501],\n",
       "         [12.8666, 10.9578,  7.6026,  ...,  7.9113,  7.8484,  7.1011],\n",
       "         ...,\n",
       "         [ 5.2729,  5.3784,  2.5678,  ...,  1.8604,  0.6186, -2.2679],\n",
       "         [ 6.4986,  5.0265,  3.4901,  ...,  1.9698,  0.2080, -2.4857],\n",
       "         [26.8334, 23.0717, 20.2516,  ..., 16.7327, 16.7596, 13.8943]]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b8ffd946-3460-4cea-aa44-f7a13257796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 51865])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b302e587-f60f-4956-a791-aa660e2fb9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2667,  0.1475],\n",
       "        [-2.0751,  1.6884],\n",
       "        [ 9.0733, 15.3917],\n",
       "        [12.4683,  5.3053],\n",
       "        [24.3392, 23.3065],\n",
       "        [23.8486, 20.8278],\n",
       "        [26.3616, 22.5004],\n",
       "        [26.4366, 22.7905],\n",
       "        [25.8284, 22.6519],\n",
       "        [25.1259, 22.4605],\n",
       "        [21.4763, 21.5335],\n",
       "        [22.7957, 26.3061],\n",
       "        [ 3.1814,  7.4455],\n",
       "        [ 4.8735,  9.7000],\n",
       "        [24.4710, 23.7138]], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits[0, :, [1338, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "44980d66-3623-48b0-8fd2-9ccc9fa68d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_processor = NoRepeatNGramLogitsProcessor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f3f9276b-6fa6-44f0-bba1-4f08c7088638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7393,  0.1077,  3.5260,  ...,  1.8835,  2.4299,  3.7835],\n",
       "         [-5.8741, -8.5107, -5.6013,  ..., -5.7708, -5.8595, -4.3501],\n",
       "         [12.8666, 10.9578,  7.6026,  ...,  7.9113,  7.8484,  7.1011],\n",
       "         ...,\n",
       "         [ 5.2729,  5.3784,  2.5678,  ...,  1.8604,  0.6186, -2.2679],\n",
       "         [ 6.4986,  5.0265,  3.4901,  ...,  1.9698,  0.2080, -2.4857],\n",
       "         [26.8334, 23.0717, 20.2516,  ..., 16.7327, 16.7596, 13.8943]]],\n",
       "       device='mps:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, sequence_length, vocab_size = outputs.logits.shape\n",
    "\n",
    "outputs_logits = outputs.logits.clone()\n",
    "\n",
    "list_processed_logits = []\n",
    "for idx in range(sequence_length):\n",
    "    list_processed_logits.append(logit_processor(input_ids=inputs[\"labels\"][:, :idx+1], scores=outputs_logits[:, idx, :]).reshape(batch_size, 1, vocab_size))\n",
    "\n",
    "y = torch.cat(list_processed_logits, dim=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a08c4ed5-8e92-4566-b0ff-f0922523bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 51865])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5f9e9791-26b7-4c5b-9b1f-d134f903632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2667,  0.1475],\n",
       "        [-2.0751,  1.6884],\n",
       "        [ 9.0733, 15.3917],\n",
       "        [12.4683,  5.3053],\n",
       "        [24.3392, 23.3065],\n",
       "        [23.8486, 20.8278],\n",
       "        [26.3616, 22.5004],\n",
       "        [26.4366, 22.7905],\n",
       "        [25.8284, 22.6519],\n",
       "        [25.1259, 22.4605],\n",
       "        [21.4763, 21.5335],\n",
       "        [22.7957, 26.3061],\n",
       "        [ 3.1814,  7.4455],\n",
       "        [ 4.8735,  9.7000],\n",
       "        [24.4710, 23.7138]], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits[0, :, [1338, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a9a08af3-a318-414b-839e-f0fd9cde9beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.3757, -2.9258, -0.3346,  ..., -4.5569, -2.7298, -0.9344],\n",
       "         [22.2727, 27.1006, 19.2470,  ..., 19.7796, 19.2167, 15.9238]]],\n",
       "       device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, [1338, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "efdda268-901b-49d6-91e2-a0b29f5cdff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='mps:0')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs.logits == y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d24c77-9d4b-4853-b919-13fc27d92a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2048ad6d-9d3e-4202-a9e4-a1a9b3edfbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, sequence_length, vocab_size = outputs.logits.shape\n",
    "\n",
    "outputs_logits = outputs.logits.clone()\n",
    "\n",
    "for idx in range(sequence_length):\n",
    "    logit_processor(input_ids=inputs[\"labels\"][:, :idx+1], scores=outputs_logits[:, idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eeeedd22-0e27-4bd2-8f22-94c099b58298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2667,  0.1475],\n",
       "        [-2.0751,  1.6884],\n",
       "        [ 9.0733, 15.3917],\n",
       "        [12.4683,  5.3053],\n",
       "        [   -inf, 23.3065],\n",
       "        [   -inf, 20.8278],\n",
       "        [   -inf, 22.5004],\n",
       "        [   -inf, 22.7905],\n",
       "        [   -inf, 22.6519],\n",
       "        [   -inf, 22.4605],\n",
       "        [   -inf, 21.5335],\n",
       "        [   -inf, 26.3061],\n",
       "        [   -inf,  7.4455],\n",
       "        [   -inf,  9.7000],\n",
       "        [   -inf, 23.7138]], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_logits[0, :, [1338, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1c077-02a9-416a-b660-91e4ea6641b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c542242-4d8c-4665-90d5-01cf342969ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
