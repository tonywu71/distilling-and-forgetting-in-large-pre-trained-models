{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37324580-06a0-4be8-8194-70b8f85523c3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb50902-acec-4fe8-9681-d944478c0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba60d42-4585-4578-b142-c703099bd091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Tony/Other Docs/distilling-and-forgetting-in-large-pre-trained-models\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b422c568-7362-4361-8052-1b78696a1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from dataloader.dataloader import gen_from_dataset\n",
    "from dataloader.dataset_for_evaluation.ami_test import AMITestSet\n",
    "from evaluation.string_edit_metrics import get_string_edit_metrics\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcde89-6579-4200-bb8c-f0d0f618971a",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb40d39-119f-4c6a-b960-99dc206d6350",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44a8a39-776d-4177-ae70-33d876edefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"english\", task=\"transcribe\")\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")  # type: ignore\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "whisper_norm = processor.tokenizer._normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482c627-cd0a-41ba-8e61-96bb22b02956",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2bcf1c7-5715-4c7e-8537-4dd0c06233e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `CACHE_DIR_AMI` environment variable not set. Using default cache directory.\n"
     ]
    }
   ],
   "source": [
    "ds_group = AMITestSet(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af0dc52-2982-4376-ba18-6baf48666b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.iterable_dataset.IterableDataset at 0x28ccdf4c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds_group.str2dataset[\"ami_test\"]\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514824b-98c9-407e-a4ff-e800c3aaba75",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81ed08a2-7488-4eda-858e-b72410c214ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_asr = pipeline(task=\"automatic-speech-recognition\",\n",
    "                       model=model,\n",
    "                       tokenizer=processor.tokenizer,  # type: ignore\n",
    "                       feature_extractor=processor.feature_extractor,  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157d5ba-883a-4ed9-9384-695eaeda6ecb",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13c4788-c27f-47ee-9acc-3d65d519fd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlmi-dissertation/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (448) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# Create placeholders for the predictions and references:\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for out in whisper_asr(gen_from_dataset(ds),\n",
    "                       batch_size=4,\n",
    "                       generate_kwargs={\"num_beams\": 1}):  # type: ignore\n",
    "    if count > 100:\n",
    "        break\n",
    "    \n",
    "    ref = whisper_norm(out[\"reference\"][0])\n",
    "    pred = whisper_norm(out[\"text\"])\n",
    "\n",
    "    if not ref.strip():\n",
    "        continue  # skip empty references to avoid error in WER computation\n",
    "\n",
    "    predictions.append(pred)\n",
    "    references.append(ref)\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9f847c-f9ce-4c40-8273-09f0159289ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you  |  yeah\n",
      "yeah we are going to meet up  |  yeah we are going to meet up\n",
      "we are not even there yet  |  i mean we are not even there yet\n",
      "yeah  |  yeah\n",
      "yeah  |  yeah yeah\n",
      "monday afternoon  |  monday afternoon\n",
      "you use my pen  |  get to use my pen\n",
      "yeah  |  yeah yeah\n",
      "everything  |  everything\n",
      "yeah that is what we need  |  yeah that is what we need yeah\n",
      "it is very scary  |  3 is good though\n",
      "okay  |  yeah\n",
      "he messed up  |  you better start\n",
      "we can always decide that  |  we can always decide then i mean yeah\n",
      "he is so .  |  you saw it yeah\n",
      "yeah  |  yeah true\n",
      "well it is kind of like .  |  well 0 dear\n",
      "i mean  |  i mean\n",
      "thanks for watching  |  yeah\n",
      "0 i do not know  |  0 i do not know\n",
      "yeah  |  yeah\n",
      "thank you  |  i th\n",
      "that is cool  |  that is cool\n",
      "so even if it does not work you can jiggery poke it around and make it work  |  so even if it does not work you can jiggery pokery around and make it work\n",
      "but most will probably want to go with the faults  |  but most will probably want to go with defaults\n",
      "you open the window you re through it you might click you know close it against right away  |  you open the window you read through it you might click on you know close it again straight away\n",
      "ok  |  okay\n",
      "down  |  yeah\n",
      "i mean if you already started on doing stuff i guess we can .  |  i mean if you already started on on doing your stuff i guess we can\n",
      "thank you  |  yeah\n",
      "thanks  |  yeah\n",
      "i do not know  |  i do not know\n",
      "so what do we need to talk about  |  so what do we need to talk about\n",
      "we do not we do not actually sure what the user wants for that but it is interesting so why do not give it time  |  we do not we are not actually sure what the user wants with it but it is interesting so why not give it to him\n",
      "probably just 2 or 3 of whatever they are looking at like dialog next and whatever their positions  |  probably just 2 or 2 or 3 of whatever whatever they are looking at like dialog acts to whatever characterisations\n",
      "but i mean this week is only 2 days all well whatever it was last week then  |  but well whatever it was last week then\n",
      "that is a good point  |  that is a good point\n",
      "them  |  done\n",
      "actually is the code accessible like the gui stuff that you have done  |  actually is the code accessible like the the gui stuff that you have done yeah\n",
      "will not be the whole thing then  |  will not be the whole thing then\n",
      "so if there is a deal so it will be a choice from the manual but yeah exactly  |  so if there is a de so it will be a choice from the menu but yeah exactly\n",
      "i am zille stau  |  mozilla style\n",
      "yeah just in case  |  yeah just in case\n",
      "well it might come to us because we start playing with it  |  well it might come to us as we start playing with it\n",
      "thank you  |  okay\n",
      "i do not know  |  okay yeah\n",
      "okay  |  okay\n",
      "you  |  yeah\n",
      "  |  huh\n",
      "yeah it is changed the contents of the same window like from transcription to summary  |  yeah change the contents of the same window like from transcription to summary\n",
      "yeah somebody else else  |  yeah somebody will answer it\n",
      "yeah  |  yeah\n",
      "okay yeah i am curious  |  okay yeah yeah okay right\n",
      "into work  |  into what\n",
      "ok  |  okay\n",
      "maybe you should make it exist then  |  maybe you should make it exist then\n",
      "no  |  no\n",
      "you are so funny  |  so\n",
      "what monday  |  what monday\n",
      "which is going to get  |  which is quite good\n",
      "bye  |  0 yeah\n",
      "that is quite slow i am really  |  that is quite slow really\n",
      "yeah that is true yeah  |  yeah that is true yeah\n",
      "for the weekend  |  or the week\n",
      "the progress of parties june 23rd february  |  the progress report is due on 23rd february\n",
      "to the end  |  does it\n",
      "that is pretty serious is not it  |  that is pretty bizarre is not it\n",
      "well it will be still limited version of the next query  |  well it will be you know still limited version of you know the next query search\n",
      "yeah  |  yeah\n",
      "so it will be like all the pieces of that topic  |  so it will be like all the cases of that topic\n",
      "whether you are going to get us access into that  |  whether he was going to get us access into the\n",
      "and for the time for the gui  |  a prototype with a gui\n",
      "coming coming seeing it out one bit of time running it does it work yeah  |  commen commenting it out one bit at a time and running it does it work yeah\n",
      "less you can not hear it probably or something  |  unless you can not hear it properly or something\n",
      "yeah  |  yeah\n",
      "we have got a browser with which comes up automatically with the transcription box and the topics  |  we have go a browser we have which comes up automatically with a transcription box and the topics\n",
      "so what time do we say  |  so what time do we say\n",
      "but then there is a problem with a lot of windows popping up again  |  yeah but then then there is the problem with a lot of windows popping up again\n",
      "i actually have not done anything this week because this week has been manic  |  i actually have not done anything this week cause this week has been manic\n",
      "you did a ta da k e for it  |  you do need to have a key for it\n",
      "all right  |  alright\n",
      "huh  |  0\n",
      "that is nice  |  that is nice\n",
      "i guess that .  |  i guess that yeah\n",
      "you  |  yeah\n",
      "  |  yeah\n",
      "i think  |  i think yeah\n",
      "so i guess that is what we need to inform  |  so i guess that is what we need to aim for\n",
      "yeah yeah  |  yeah yeah\n",
      "you know whatever you want to do then just send me an email  |  you know whatever you want to do then just send me an email\n",
      "have all these sort of using the same window and so we can put a lot of and probably mutually what do distinctly  |  have all these sort of using the same window and so we can put a lot of and probably mutually whate distinct yeah\n",
      "i can tell you what other than this bt  |  i could tell you what i done in s p 2\n",
      "or 330  |  or 330\n",
      "is  |  he is close\n",
      "0 you should be rotti  |  0 you should be able to\n",
      "so  |  so\n",
      "and have a right click from for these various things to happen  |  have a right click from for these various things to yeah\n",
      "yeah  |  yeah\n",
      "you  |  damn\n",
      "that is true yeah it is more intuitive really is not it  |  that is true yeah it is more intuitive really is not it\n",
      "what  |  what\n"
     ]
    }
   ],
   "source": [
    "for pred, ref in zip(predictions, references):\n",
    "    print(pred, \" | \", ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3ab6f-d53d-4409-8065-2258b7d56ca9",
   "metadata": {},
   "source": [
    "## Compute string edit metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3329428-deee-480f-9dee-e8174f42d02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3147208121827411"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53cbab69-30ed-4788-be0d-037130fef0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wer': 0.3147208121827411,\n",
       " 'sub': 0.17597292724196278,\n",
       " 'del': 0.06937394247038917,\n",
       " 'ins': 0.06937394247038917}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_string_edit_metrics(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f2700-71ca-46a0-816b-2441885afd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
